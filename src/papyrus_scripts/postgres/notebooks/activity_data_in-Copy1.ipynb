{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b9205",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T14:59:40.841958Z",
     "start_time": "2022-05-30T14:59:40.328162Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/code')\n",
    "\n",
    "from database.models import (Protein, Organism, Classification, Molecule, Activity, ActivityType, Source, Quality, CID)\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import scoped_session\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import os\n",
    "import glob\n",
    "from rdkit import Chem\n",
    "from razi.rdkit_postgresql.functions import morganbv_fp\n",
    "\n",
    "import pandas as pd\n",
    "# import modin.pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import multiprocessing\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "def get_db_session():\n",
    "    engine = create_engine(\n",
    "        os.environ.get('SQLALCHEMY_URL'), convert_unicode=True,\n",
    "        pool_recycle=3600, pool_size=10)\n",
    "    db_session = scoped_session(sessionmaker(\n",
    "        autocommit=False, autoflush=False, bind=engine))\n",
    "    \n",
    "    return db_session\n",
    "\n",
    "\n",
    "def get_or_create(session, model, **kwargs):\n",
    "    instance = session.query(model).filter_by(**kwargs).first()\n",
    "    if instance:\n",
    "        return instance\n",
    "    else:\n",
    "        instance = model(**kwargs)\n",
    "        session.add(instance)\n",
    "        session.flush()\n",
    "        session.refresh(instance)\n",
    "        return instance\n",
    "    \n",
    "def get_or_instance(session, model, **kwargs):\n",
    "    instance = session.query(model).filter_by(**kwargs).first()\n",
    "    if instance:\n",
    "        return (False, instance)\n",
    "    else:\n",
    "        instance = model(**kwargs)\n",
    "        return (True, instance)\n",
    "    \n",
    "def sanitize_and_split(row, length, spl=';'):\n",
    "    split = [v.rstrip() for v in str(row).split(spl)]\n",
    "    if len(split)!= length:\n",
    "        split = [split[0] for i in range(0,length)]\n",
    "    \n",
    "    split = [None if x == '' else x for x in split]\n",
    "    \n",
    "    return split\n",
    "\n",
    "\n",
    "class TypeDecoder(json.JSONDecoder):\n",
    "    \"\"\"Custom json decoder to support types as values.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Simple json decoder handling types as values.\"\"\"\n",
    "        json.JSONDecoder.__init__(self, object_hook=self.object_hook, *args, **kwargs)\n",
    "\n",
    "    def object_hook(self, obj):\n",
    "        \"\"\"Handle types.\"\"\"\n",
    "        if '__type__' not in obj:\n",
    "            return obj\n",
    "        module = obj['__type__']['module']\n",
    "        type_ = obj['__type__']['type']\n",
    "        if module == 'builtins':\n",
    "            return getattr(__builtins__, type_)\n",
    "        loaded_module = importlib.import_module(module)\n",
    "        return getattr(loaded_module, type_)\n",
    "    \n",
    "\n",
    "\n",
    "dtype_file = '../.data/papyrus/05.5/data_types.json'\n",
    "activity_data = '../.data/papyrus/05.5/05.5_combined_set_without_stereochemistry.tsv.xz'\n",
    "protein_data = '../.data/papyrus/05.5/05.5_combined_set_protein_targets.tsv.xz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77a85c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T14:59:40.855644Z",
     "start_time": "2022-05-30T14:59:40.844979Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(dtype_file, 'r') as jsonfile:\n",
    "        dtypes = json.load(jsonfile, cls=TypeDecoder)['papyrus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b7969",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-26T15:29:44.722093Z",
     "start_time": "2022-05-26T15:29:44.569082Z"
    }
   },
   "outputs": [],
   "source": [
    "protein_df = pd.read_csv(protein_data, sep='\\t', dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d3070",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-26T15:29:45.571112Z",
     "start_time": "2022-05-26T15:29:45.561539Z"
    }
   },
   "outputs": [],
   "source": [
    "organisms = list(set(protein_df['Organism']))\n",
    "classifications = []\n",
    "for cstr in protein_df['Classification']:\n",
    "    classifications.extend(str(cstr).split('->'))\n",
    "\n",
    "classes = list(set(classifications))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303809f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-26T15:30:03.792239Z",
     "start_time": "2022-05-26T15:29:46.732740Z"
    }
   },
   "outputs": [],
   "source": [
    "db_session = get_db_session()\n",
    "\n",
    "rows = []\n",
    "\n",
    "for i, row in protein_df.iterrows():\n",
    "    organism = get_or_create(session=db_session, model=Organism, organism=row['Organism'])\n",
    "    classifications_list = str(row['Classification']).split('->')\n",
    "    classifications = [get_or_create(session=db_session, model=Classification, classification=c) for c in classifications_list]\n",
    "    \n",
    "    review_mapping = {'reviewed':1, 'Unreviewed':0, 'unreviewed':0}\n",
    "    \n",
    "    prot = Protein(\n",
    "        target_id = row['target_id'],\n",
    "        HGNC_symbol = str(row['HGNC_symbol']),\n",
    "        uniprot_id = row['UniProtID'],\n",
    "        reviewed = review_mapping[row['Status']],\n",
    "        organism = organism.id,\n",
    "        length = row['Length'],\n",
    "        sequence = row['Sequence'], \n",
    "        classifications = classifications\n",
    "    )\n",
    "    \n",
    "    rows.append(prot)\n",
    "    \n",
    "db_session.add_all(rows)\n",
    "db_session.commit()\n",
    "db_session.remove()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ca3087",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T14:59:43.893584Z",
     "start_time": "2022-05-30T14:59:43.877084Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_cid_arr(sources_list, cids_list, session):\n",
    "    sources_cids_list = list(zip(sources_list, cids_list))\n",
    "    cids = [      \n",
    "        get_or_create(session=session, \n",
    "                      model=CID, \n",
    "                      cid=c[1], \n",
    "                      source=get_or_create(session=session, \n",
    "                                           model=Source, \n",
    "                                           source=c[0]).source) for c in sources_cids_list]\n",
    "    return cids\n",
    "\n",
    "\n",
    "def get_molecule(session, row_tup, cids):\n",
    "#     out = []\n",
    "    mol = Chem.MolFromSmiles(row_tup.SMILES)\n",
    "    fp = morganbv_fp(row_tup.SMILES)\n",
    "\n",
    "    # change this to use InChI and/or SMILES\n",
    "    smiles = Chem.CanonSmiles(row_tup.SMILES)\n",
    "    created, molecule = get_or_instance(session=session,model=Molecule,smiles=smiles,inchi=row_tup.InChI)\n",
    "\n",
    "    for cid in cids:\n",
    "        if cid not in molecule.cids:\n",
    "            molecule.cids.append(cid)\n",
    "\n",
    "    if created:\n",
    "        molecule.smiles=smiles\n",
    "        molecule.mol=mol\n",
    "        molecule.inchi_key=row_tup.InChIKey\n",
    "        molecule.inchi=row_tup.InChI\n",
    "        molecule.inchi_auxinfo=row_tup.InChI_AuxInfo\n",
    "        molecule.fp=fp\n",
    "        molecule.connectivity=row_tup.connectivity\n",
    "        session.add(molecule)\n",
    "        session.flush()\n",
    "        session.refresh(molecule)\n",
    "        \n",
    "    return molecule\n",
    "\n",
    "def get_activity_dicts(row):\n",
    "    slice_list = []\n",
    "    if ';' in str(row.pchembl_value):\n",
    "\n",
    "        pchembl_values = [v.rstrip() for v in row.pchembl_value.split(';')]\n",
    "        length = len(pchembl_values)\n",
    "\n",
    "        aids = sanitize_and_split(row=row.AID,length=length)        \n",
    "        doc_ids = sanitize_and_split(row=row.all_doc_ids,length=length)\n",
    "        years = sanitize_and_split(row=row.all_years,length=length)\n",
    "        type_IC50s = sanitize_and_split(row=row.type_IC50,length=length)         \n",
    "        type_EC50s = sanitize_and_split(row=row.type_EC50,length=length)\n",
    "        type_KDs = sanitize_and_split(row=row.type_KD,length=length)\n",
    "        type_Kis = sanitize_and_split(row=row.type_Ki,length=length)\n",
    "\n",
    "        for j in range(0, len(pchembl_values)):\n",
    "            update_dict = {\n",
    "                'pchembl_value': pchembl_values[j],\n",
    "                'AID': aids[j],\n",
    "                'doc_id': doc_ids[j],\n",
    "                'Year': years[j],\n",
    "                'type_IC50': type_IC50s[j],\n",
    "                'type_EC50': type_EC50s[j],\n",
    "                'type_KD': type_KDs[j],\n",
    "                'type_Ki': type_Kis[j]\n",
    "            }\n",
    "            row_copy = copy(row._asdict())\n",
    "\n",
    "            row_copy.update(update_dict)\n",
    "\n",
    "            slice_list.append(row_copy)\n",
    "\n",
    "    else:\n",
    "        slice_list.append(row._asdict())\n",
    "            \n",
    "    return slice_list\n",
    "\n",
    "def process_row(row, session):\n",
    "    \n",
    "    activity_type_map = {\n",
    "        '1000':'IC50',\n",
    "        '0100':'EC50',\n",
    "        '0010':'KD',\n",
    "        '0001':'Ki',\n",
    "        '0000':'other',\n",
    "    }\n",
    "    \n",
    "    rows = []\n",
    "    sources_list = row.source.split(';')\n",
    "    cids_list = row.CID.split(';')\n",
    "    cids = get_cid_arr(sources_list, cids_list, session)\n",
    "\n",
    "    rows.extend(cids)\n",
    "\n",
    "    molecule = get_molecule(session=session, row_tup=row, cids=cids)\n",
    "\n",
    "    rows.append(molecule)\n",
    "\n",
    "    qc,quality = get_or_instance(session=session, model=Quality, quality=row.Quality)\n",
    "    if qc: rows.append(quality)\n",
    "    qid = quality.id\n",
    "\n",
    "    tc,target_id = get_or_instance(session=session, model=Protein, target_id=row.target_id)\n",
    "    if tc: rows.append(target_id)\n",
    "    tid = target_id.target_id\n",
    "\n",
    "    molecule_id = molecule.id\n",
    "\n",
    "    slice_list = get_activity_dicts(row)\n",
    "\n",
    "    for s in slice_list:\n",
    "\n",
    "        a = f\"{s['type_IC50']}{s['type_EC50']}{s['type_KD']}{s['type_Ki']}\"\n",
    "        activity_type_str = activity_type_map[a]\n",
    "\n",
    "        activity_type = get_or_create(session=session, model=ActivityType, type=activity_type_str).id\n",
    "\n",
    "        try:\n",
    "            y = int(s['Year'])\n",
    "        except:\n",
    "            y = None\n",
    "\n",
    "        if str(s['doc_id']) in ['nan', 'NaN']:\n",
    "            doc_id = None\n",
    "        else:\n",
    "            doc_id = str(s['doc_id'])\n",
    "\n",
    "        activity = Activity(\n",
    "            papyrus_activity_id=s['Activity_ID'],\n",
    "            quality=qid,\n",
    "            target_id=tid,\n",
    "            molecule_id = molecule_id,\n",
    "            accession=s['accession'],\n",
    "            protein_type=s['Protein_Type'],\n",
    "            aid = s['AID'],\n",
    "            doc_id = doc_id,\n",
    "            year = y,\n",
    "            type = activity_type, \n",
    "            relation = s['relation'],\n",
    "            pchembl_value = s['pchembl_value'],\n",
    "            pchembl_value_mean = s['pchembl_value_Mean'],\n",
    "            pchembl_value_stdev = s['pchembl_value_StdDev'],\n",
    "            pchembl_value_SEM = s['pchembl_value_SEM'],\n",
    "            pchembl_value_n = s['pchembl_value_N'],\n",
    "            pchembl_value_median = s['pchembl_value_Median'],\n",
    "            pchembl_value_mad = s['pchembl_value_MAD'],   \n",
    "        )\n",
    "\n",
    "        rows.append(activity)\n",
    "        \n",
    "    return rows\n",
    "            \n",
    "\n",
    "def process_activity_frame(df):\n",
    "#     df_obj = df.select_dtypes(['object'])\n",
    "    db_session = get_db_session()\n",
    "    rows = []\n",
    "    \n",
    "\n",
    "    print('processing frame')\n",
    "    \n",
    "    \n",
    "    \n",
    "#     pool = multiprocessing.Pool(5)\n",
    "#     results = pool.imap_unordered(process_row, list(zip([row for row in df.itertuples()], db_session)), chunksize=1000)\n",
    "\n",
    "    ddf = dd.from_pandas(df, chunksize=1000)\n",
    "    \n",
    "    # do this bit in parallel?\n",
    "    for row in tqdm(ddf.itertuples()):\n",
    "        rows.extend(process_row(row, db_session))\n",
    "        dd.compute()\n",
    "            \n",
    "    print('processing complete')\n",
    "            \n",
    "    del(df)\n",
    "    db_session.add_all(rows)\n",
    "    print('committing data')\n",
    "    db_session.commit()\n",
    "    db_session.close()\n",
    "    db_session.remove()\n",
    "    gc.collect()\n",
    "            \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738fc5a6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-05-30T14:59:37.597Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reader = pd.read_csv(activity_data, sep='\\t', compression='xz', chunksize = 10000, iterator=True, dtype=dtypes)\n",
    "\n",
    "for (i,df) in enumerate(reader):\n",
    "    # process each data frame\n",
    "    print(f'processing chunk {i}')\n",
    "    process_activity_frame(df)\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6208dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "ddata = dd.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a240a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = os.environ.get('SQLALCHEMY_URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ddbe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d7c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/code')\n",
    "\n",
    "from database.models import (Protein, Organism, Classification, Molecule, Activity, ActivityType, Source, Quality, CID)\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import scoped_session\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import multiprocessing\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "def get_db_session():\n",
    "    engine = create_engine(\n",
    "        os.environ.get('SQLALCHEMY_URL'), convert_unicode=True,\n",
    "        pool_recycle=3600, pool_size=10)\n",
    "    db_session = scoped_session(sessionmaker(\n",
    "        autocommit=False, autoflush=False, bind=engine))\n",
    "    \n",
    "    return db_session\n",
    "\n",
    "\n",
    "class TypeDecoder(json.JSONDecoder):\n",
    "    \"\"\"Custom json decoder to support types as values.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Simple json decoder handling types as values.\"\"\"\n",
    "        json.JSONDecoder.__init__(self, object_hook=self.object_hook, *args, **kwargs)\n",
    "\n",
    "    def object_hook(self, obj):\n",
    "        \"\"\"Handle types.\"\"\"\n",
    "        if '__type__' not in obj:\n",
    "            return obj\n",
    "        module = obj['__type__']['module']\n",
    "        type_ = obj['__type__']['type']\n",
    "        if module == 'builtins':\n",
    "            return getattr(__builtins__, type_)\n",
    "        loaded_module = importlib.import_module(module)\n",
    "        return getattr(loaded_module, type_)\n",
    "    \n",
    "\n",
    "# get neccessary molecule data as list of tuples\n",
    "session = get_db_session()\n",
    "mol_data = session.query(Molecule.id, Molecule.smiles, Molecule.inchi_auxinfo).all()\n",
    "mol_ids = {}\n",
    "for t in tqdm(mol_data):\n",
    "    combo = f'{t[1]}_{t[2]}'\n",
    "    if not combo in mol_ids.keys():\n",
    "        mol_ids[combo] = [t[0]]\n",
    "    else:\n",
    "        mol_ids[combo].append(t[0])\n",
    "\n",
    "# add activity types\n",
    "types = ['EC50', 'IC50', 'KD', 'Ki', 'other']\n",
    "# atypes = [ActivityType(type=t) for t in types]\n",
    "# session.add_all(atypes)\n",
    "# session.commit()\n",
    "\n",
    "# return ids for activity types as dict\n",
    "types = dict(session.query(ActivityType.type, ActivityType.id).all())\n",
    "\n",
    "# add qualities\n",
    "qualities = [\"High\",\"Low\",\"Medium\",\"Medium;Low\",\"Low;Medium\"]\n",
    "# qs = [Quality(quality=q) for q in qualities]\n",
    "# session.add_all(qs)\n",
    "# session.commit()\n",
    "\n",
    "# return ids for qualities as list of tuples\n",
    "qualities = dict(session.query(Quality.quality,Quality.id).all())\n",
    "\n",
    "# get all targets - probably don't need this \n",
    "targets = session.query(Protein.target_id).all()\n",
    "\n",
    "pchembl_val_list = lambda pchembl_values: ([v.rstrip() for v in pchembl_values.split(';')])\n",
    "smiles = lambda smi: (Chem.CanonSmiles(smi))\n",
    "\n",
    "converters = {\n",
    "    'SMILES':smiles,\n",
    "    'pchembl_value':pchembl_val_list\n",
    "}\n",
    "\n",
    "dtype_file = '../.data/papyrus/05.5/data_types.json'\n",
    "activity_data = '../.data/papyrus/05.5/05.5_combined_set_without_stereochemistry.tsv.xz'\n",
    "\n",
    "with open(dtype_file, 'r') as jsonfile:\n",
    "        dtypes = json.load(jsonfile, cls=TypeDecoder)['papyrus']\n",
    "\n",
    "# all columns needed to process activity data\n",
    "activity_columns = ['Activity_ID', 'SMILES', 'InChI_AuxInfo', 'accession', 'Protein_Type', 'AID', 'doc_id',\n",
    "                    'Year', 'type_IC50', 'type_EC50', 'type_KD', 'type_Ki', 'relation', 'pchembl_value',\n",
    "                    'pchembl_value_Mean', 'pchembl_value_StdDev', 'pchembl_value_SEM', 'pchembl_value_N', \n",
    "                    'pchembl_value_Median', 'pchembl_value_MAD','Quality'\n",
    "                   ]\n",
    "\n",
    "activity_reader = pd.read_csv(activity_data, \n",
    "                     sep='\\t', \n",
    "                     compression='xz', \n",
    "                     chunksize = 10000, \n",
    "                     iterator=True, \n",
    "                     dtype=dtypes,\n",
    "                     converters=converters,\n",
    "                     usecols=activity_columns\n",
    "                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ee9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_columns = ['Activity_ID', 'SMILES', 'InChI_AuxInfo', 'accession', 'Protein_Type', 'AID', 'doc_id',\n",
    "                    'Year', 'type_IC50', 'type_EC50', 'type_KD', 'type_Ki', 'relation', 'pchembl_value',\n",
    "                    'pchembl_value_Mean', 'pchembl_value_StdDev', 'pchembl_value_SEM', 'pchembl_value_N', \n",
    "                    'pchembl_value_Median', 'pchembl_value_MAD','Quality', 'all_doc_ids', 'all_years'\n",
    "                   ]\n",
    "\n",
    "activity_reader = pd.read_csv(activity_data, \n",
    "                     sep='\\t', \n",
    "                     compression='xz', \n",
    "                     chunksize = 100, \n",
    "                     iterator=True, \n",
    "                     dtype=dtypes,\n",
    "                     converters=converters,\n",
    "                     usecols=activity_columns\n",
    "                    )\n",
    "\n",
    "for i,df in enumerate(activity_reader):\n",
    "    df.head()\n",
    "    if i>0:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408385db",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlen = lambda x: (len(x))\n",
    "\n",
    "df['pchembl_len'] = df['pchembl_value'].map(xlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90931320",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_type_map = {\n",
    "        '1000':'IC50',\n",
    "        '0100':'EC50',\n",
    "        '0010':'KD',\n",
    "        '0001':'Ki',\n",
    "        '0000':'other',\n",
    "    }\n",
    "   \n",
    "\n",
    "# df['atype'] = df.apply(lambda x: get_atype(f\"{x['type_IC50']}{x['type_EC50']}{x['type_KD']}{x['type_Ki']}\"), axis=1)\n",
    "\n",
    "multis = df[df['pchembl_len']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b9b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "multis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd797374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_and_split(row, length, spl=';'):\n",
    "    split = [v.rstrip() for v in str(row).split(spl)]\n",
    "    if len(split)!= length:\n",
    "        split = [split[0] for i in range(0,length)]\n",
    "    \n",
    "    split = [None if x == '' else x for x in split]\n",
    "    \n",
    "    return split\n",
    "\n",
    "def get_activity_dicts(row):\n",
    "    slice_list = []\n",
    "    pchembl_values = [v.rstrip() for v in row.pchembl_value]\n",
    "\n",
    "    aids = sanitize_and_split(row=row.AID,length=row.pchembl_len)        \n",
    "    doc_ids = sanitize_and_split(row=row.all_doc_ids,length=row.pchembl_len)\n",
    "    years = sanitize_and_split(row=row.all_years,length=row.pchembl_len)\n",
    "    type_IC50s = sanitize_and_split(row=row.type_IC50,length=row.pchembl_len)         \n",
    "    type_EC50s = sanitize_and_split(row=row.type_EC50,length=row.pchembl_len)\n",
    "    type_KDs = sanitize_and_split(row=row.type_KD,length=row.pchembl_len)\n",
    "    type_Kis = sanitize_and_split(row=row.type_Ki,length=row.pchembl_len)\n",
    "\n",
    "    for j in range(0, row.pchembl_len):\n",
    "        update_dict = {\n",
    "            'pchembl_value': pchembl_values[j],\n",
    "            'AID': aids[j],\n",
    "            'doc_id': doc_ids[j],\n",
    "            'Year': years[j],\n",
    "            'type_IC50': type_IC50s[j],\n",
    "            'type_EC50': type_EC50s[j],\n",
    "            'type_KD': type_KDs[j],\n",
    "            'type_Ki': type_Kis[j]\n",
    "        }\n",
    "        row_copy = copy(row._asdict())\n",
    "\n",
    "        row_copy.update(update_dict)\n",
    "\n",
    "        slice_list.append(row_copy)\n",
    "            \n",
    "    return slice_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62a2c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_rows = []\n",
    "for row in multis.itertuples():\n",
    "    updated_rows.extend(get_activity_dicts(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c709aec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sval = lambda x: (x[0])\n",
    "\n",
    "multi_expanded = pd.DataFrame(updated_rows)\n",
    "singles = df[df['pchembl_len']==1]\n",
    "singles['pchembl_value'] = singles['pchembl_value'].map(sval)\n",
    "all_expanded = pd.concat([multi_expanded, singles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccba8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_expanded['atype'] = all_expanded.apply(lambda x: get_atype(f\"{x['type_IC50']}{x['type_EC50']}{x['type_KD']}{x['type_Ki']}\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f71fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94081220",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = session.query(\n",
    " Molecule.connectivity,\n",
    " Molecule.fp,\n",
    " Molecule.id,\n",
    " Molecule.inchi,\n",
    " Molecule.inchi_auxinfo,\n",
    " Molecule.inchi_key,\n",
    " Molecule.mol,\n",
    " Molecule.smiles\n",
    "                 \n",
    "                 ).filter_by(smiles='Nc1ncnc2[nH]c(SCCOc3ccc(Cl)cc3)nc12', inchi_auxinfo='\"AuxInfo=1/1/N:12,15,11,16,8,7,20,13,10,3,2,18,5,14,1,21,19,4,17,9,6/E:(1,2)(3,4)/rA:21NCCNCSCCOCCCCClCCNCNCN/rB:s1;d2;s3;d4;s5;s6;s7;s8;s9;d10;s11;d12;s13;s13;s10d15;s5;s3s17;d18;s19;s2d20;/rC:;;;;;;;;;;;;;;;;;;;;;\"').all()\n",
    "[r._asdict() for r in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c214e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    \n",
    "    activity_type_map = {\n",
    "        '1000':'IC50',\n",
    "        '0100':'EC50',\n",
    "        '0010':'KD',\n",
    "        '0001':'Ki',\n",
    "        '0000':'other',\n",
    "    }\n",
    "    \n",
    "    mkey = f'{row.SMILES}_{row.InChI_Auxinfo}'\n",
    "    mol_ids = mol_ids[mkey]\n",
    "    qid = qualities[row.Quality]\n",
    "\n",
    "    qc,quality = get_or_instance(session=session, model=Quality, quality=row.Quality)\n",
    "    if qc: rows.append(quality)\n",
    "    qid = quality.id\n",
    "\n",
    "    tc,target_id = get_or_instance(session=session, model=Protein, target_id=row.target_id)\n",
    "    if tc: rows.append(target_id)\n",
    "    tid = target_id.target_id\n",
    "\n",
    "    molecule_id = molecule.id\n",
    "\n",
    "    slice_list = get_activity_dicts(row)\n",
    "\n",
    "    for s in slice_list:\n",
    "\n",
    "        a = f\"{s['type_IC50']}{s['type_EC50']}{s['type_KD']}{s['type_Ki']}\"\n",
    "        activity_type_str = activity_type_map[a]\n",
    "\n",
    "        activity_type = get_or_create(session=session, model=ActivityType, type=activity_type_str).id\n",
    "\n",
    "        try:\n",
    "            y = int(s['Year'])\n",
    "        except:\n",
    "            y = None\n",
    "\n",
    "        if str(s['doc_id']) in ['nan', 'NaN']:\n",
    "            doc_id = None\n",
    "        else:\n",
    "            doc_id = str(s['doc_id'])\n",
    "\n",
    "        activity = Activity(\n",
    "            papyrus_activity_id=s['Activity_ID'],\n",
    "            quality=qid,\n",
    "            target_id=tid,\n",
    "            molecule_id = molecule_id,\n",
    "            accession=s['accession'],\n",
    "            protein_type=s['Protein_Type'],\n",
    "            aid = s['AID'],\n",
    "            doc_id = doc_id,\n",
    "            year = y,\n",
    "            type = activity_type, \n",
    "            relation = s['relation'],\n",
    "            pchembl_value = s['pchembl_value'],\n",
    "            pchembl_value_mean = s['pchembl_value_Mean'],\n",
    "            pchembl_value_stdev = s['pchembl_value_StdDev'],\n",
    "            pchembl_value_SEM = s['pchembl_value_SEM'],\n",
    "            pchembl_value_n = s['pchembl_value_N'],\n",
    "            pchembl_value_median = s['pchembl_value_Median'],\n",
    "            pchembl_value_mad = s['pchembl_value_MAD'],   \n",
    "        )\n",
    "\n",
    "        rows.append(activity)\n",
    "        \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebd1562",
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441ee70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "dels = []\n",
    "for k in tqdm(mol_ids.keys()):\n",
    "    if len(mol_ids[k])>1:\n",
    "        to_del = mol_ids[k][1:]\n",
    "        dels.extend(to_del)\n",
    "#         for i in to_del:\n",
    "#             session.query(CID).filter_by(molecule_id=i).delete()\n",
    "#             session.query(Molecule).filter_by(id=i).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cids = []\n",
    "mols = []\n",
    "count = 0\n",
    "\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "for group in chunker(dels, 1000):\n",
    "    count +=1\n",
    "    print(f'{count*1000}/{len(dels)}')\n",
    "    cids.extend(session.query(CID).filter(CID.molecule_id.in_(group)).all())\n",
    "    mols.extend(session.query(Molecule).filter(Molecule.id.in_(group)).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b242158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77db789",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af6660",
   "metadata": {},
   "outputs": [],
   "source": [
    "[session.delete(cid) for cid in cids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea17ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "[session.delete(m) for m in mols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f20729",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e8de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for group in chunker(cids, 1000):\n",
    "    count +=1\n",
    "    print(f'{count*1000}/{len(cids)}')\n",
    "    [session.delete(c) for c in group]\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7156dfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19519/2591433565.py:25: SADeprecationWarning: The create_engine.convert_unicode parameter and corresponding dialect-level parameters are deprecated, and will be removed in a future release.  Modern DBAPIs support Python Unicode natively and this parameter is unnecessary.\n",
      "  engine = create_engine(\n",
      "/tmp/ipykernel_19519/2591433565.py:45: SAWarning: relationship 'Protein.classification' will copy column protein.target_id to column ProteinClassification.protein_id, which conflicts with relationship(s): 'Classification.protein' (copies protein.target_id to ProteinClassification.protein_id), 'Protein.classifications' (copies protein.target_id to ProteinClassification.protein_id). If this is not the intention, consider if these relationships should be linked with back_populates, or if viewonly=True should be applied to one or more if they are read-only. For the less common case that foreign key constraints are partially overlapping, the orm.foreign() annotation can be used to isolate the columns that should be written towards.   To silence this warning, add the parameter 'overlaps=\"classifications,protein\"' to the 'Protein.classification' relationship. (Background on this error at: https://sqlalche.me/e/14/qzyx)\n",
      "  mol_data = session.query(Molecule.id, Molecule.smiles, Molecule.inchi_auxinfo).all()\n",
      "/tmp/ipykernel_19519/2591433565.py:45: SAWarning: relationship 'Protein.classification' will copy column classification.id to column ProteinClassification.classification_id, which conflicts with relationship(s): 'Classification.protein' (copies classification.id to ProteinClassification.classification_id), 'Protein.classifications' (copies classification.id to ProteinClassification.classification_id). If this is not the intention, consider if these relationships should be linked with back_populates, or if viewonly=True should be applied to one or more if they are read-only. For the less common case that foreign key constraints are partially overlapping, the orm.foreign() annotation can be used to isolate the columns that should be written towards.   To silence this warning, add the parameter 'overlaps=\"classifications,protein\"' to the 'Protein.classification' relationship. (Background on this error at: https://sqlalche.me/e/14/qzyx)\n",
      "  mol_data = session.query(Molecule.id, Molecule.smiles, Molecule.inchi_auxinfo).all()\n",
      "/tmp/ipykernel_19519/2591433565.py:45: SAWarning: relationship 'Classification.proteins' will copy column classification.id to column ProteinClassification.classification_id, which conflicts with relationship(s): 'Classification.protein' (copies classification.id to ProteinClassification.classification_id), 'Protein.classifications' (copies classification.id to ProteinClassification.classification_id). If this is not the intention, consider if these relationships should be linked with back_populates, or if viewonly=True should be applied to one or more if they are read-only. For the less common case that foreign key constraints are partially overlapping, the orm.foreign() annotation can be used to isolate the columns that should be written towards.   To silence this warning, add the parameter 'overlaps=\"classifications,protein\"' to the 'Classification.proteins' relationship. (Background on this error at: https://sqlalche.me/e/14/qzyx)\n",
      "  mol_data = session.query(Molecule.id, Molecule.smiles, Molecule.inchi_auxinfo).all()\n",
      "/tmp/ipykernel_19519/2591433565.py:45: SAWarning: relationship 'Classification.proteins' will copy column protein.target_id to column ProteinClassification.protein_id, which conflicts with relationship(s): 'Classification.protein' (copies protein.target_id to ProteinClassification.protein_id), 'Protein.classifications' (copies protein.target_id to ProteinClassification.protein_id). If this is not the intention, consider if these relationships should be linked with back_populates, or if viewonly=True should be applied to one or more if they are read-only. For the less common case that foreign key constraints are partially overlapping, the orm.foreign() annotation can be used to isolate the columns that should be written towards.   To silence this warning, add the parameter 'overlaps=\"classifications,protein\"' to the 'Classification.proteins' relationship. (Background on this error at: https://sqlalche.me/e/14/qzyx)\n",
      "  mol_data = session.query(Molecule.id, Molecule.smiles, Molecule.inchi_auxinfo).all()\n",
      "100%|██████████████████████████████████████████████████████| 1503703/1503703 [00:05<00:00, 253233.99it/s]\n",
      "100%|█████████████████████████████████████████████████████| 1425005/1425005 [00:01<00:00, 1268390.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/78698\n",
      "2000/78698\n",
      "3000/78698\n",
      "4000/78698\n",
      "5000/78698\n",
      "6000/78698\n",
      "7000/78698\n",
      "8000/78698\n",
      "9000/78698\n",
      "10000/78698\n",
      "11000/78698\n",
      "12000/78698\n",
      "13000/78698\n",
      "14000/78698\n",
      "15000/78698\n",
      "16000/78698\n",
      "17000/78698\n",
      "18000/78698\n",
      "19000/78698\n",
      "20000/78698\n",
      "21000/78698\n",
      "22000/78698\n",
      "23000/78698\n",
      "24000/78698\n",
      "25000/78698\n",
      "26000/78698\n",
      "27000/78698\n",
      "28000/78698\n",
      "29000/78698\n",
      "30000/78698\n",
      "31000/78698\n",
      "32000/78698\n",
      "33000/78698\n",
      "34000/78698\n",
      "35000/78698\n",
      "36000/78698\n",
      "37000/78698\n",
      "38000/78698\n",
      "39000/78698\n",
      "40000/78698\n",
      "41000/78698\n",
      "42000/78698\n",
      "43000/78698\n",
      "44000/78698\n",
      "45000/78698\n",
      "46000/78698\n",
      "47000/78698\n",
      "48000/78698\n",
      "49000/78698\n",
      "50000/78698\n",
      "51000/78698\n",
      "52000/78698\n",
      "53000/78698\n",
      "54000/78698\n",
      "55000/78698\n",
      "56000/78698\n",
      "57000/78698\n",
      "58000/78698\n",
      "59000/78698\n",
      "60000/78698\n",
      "61000/78698\n",
      "62000/78698\n",
      "63000/78698\n",
      "64000/78698\n",
      "65000/78698\n",
      "66000/78698\n",
      "67000/78698\n",
      "68000/78698\n",
      "69000/78698\n",
      "70000/78698\n",
      "71000/78698\n",
      "72000/78698\n",
      "73000/78698\n",
      "74000/78698\n",
      "75000/78698\n",
      "76000/78698\n",
      "77000/78698\n",
      "78000/78698\n",
      "79000/78698\n",
      "100/78698\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 70>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(mols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     73\u001b[0m [session\u001b[38;5;241m.\u001b[39mdelete(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m group]\n\u001b[0;32m---> 74\u001b[0m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36mcommit\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/papyrus/lib/python3.8/site-packages/sqlalchemy/orm/session.py:1435\u001b[0m, in \u001b[0;36mSession.commit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autobegin():\n\u001b[1;32m   1433\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m sa_exc\u001b[38;5;241m.\u001b[39mInvalidRequestError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo transaction is begun.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1435\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transaction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfuture\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/papyrus/lib/python3.8/site-packages/sqlalchemy/orm/session.py:829\u001b[0m, in \u001b[0;36mSessionTransaction.commit\u001b[0;34m(self, _to_root)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_active(prepared_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m PREPARED:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnested:\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m conn, trans, should_commit, autoclose \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connections\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    834\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/conda/envs/papyrus/lib/python3.8/site-packages/sqlalchemy/orm/session.py:808\u001b[0m, in \u001b[0;36mSessionTransaction._prepare_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39m_is_clean():\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 808\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    810\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mFlushError(\n\u001b[1;32m    811\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOver 100 subsequent flushes have occurred within \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msession.commit() - is an after_flush() hook \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreating new objects?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    814\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/papyrus/lib/python3.8/site-packages/sqlalchemy/orm/session.py:3367\u001b[0m, in \u001b[0;36mSession.flush\u001b[0;34m(self, objects)\u001b[0m\n\u001b[1;32m   3365\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flushing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 3367\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3368\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   3369\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flushing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/papyrus/lib/python3.8/site-packages/sqlalchemy/orm/session.py:3507\u001b[0m, in \u001b[0;36mSession._flush\u001b[0;34m(self, objects)\u001b[0m\n\u001b[1;32m   3505\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   3506\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39msafe_reraise():\n\u001b[0;32m-> 3507\u001b[0m         transaction\u001b[38;5;241m.\u001b[39mrollback(_capture_exception\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/papyrus/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py:70\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarn_only:\n\u001b[0;32m---> 70\u001b[0m         \u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexc_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_traceback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexc_tb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compat\u001b[38;5;241m.\u001b[39mpy3k \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;66;03m# emulate Py3K's behavior of telling us when an exception\u001b[39;00m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;66;03m# occurs in an exception handler.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/papyrus/lib/python3.8/site-packages/sqlalchemy/util/compat.py:207\u001b[0m, in \u001b[0;36mraise_\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    204\u001b[0m     exception\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m replace_context\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# credit to\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# as the __traceback__ object creates a cycle\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m exception, replace_context, from_, with_traceback\n",
      "File \u001b[0;32m/opt/conda/envs/papyrus/lib/python3.8/site-packages/sqlalchemy/orm/session.py:3467\u001b[0m, in \u001b[0;36mSession._flush\u001b[0;34m(self, objects)\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_on_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3467\u001b[0m     \u001b[43mflush_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3468\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   3469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_on_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/papyrus/lib/python3.8/site-packages/sqlalchemy/orm/unitofwork.py:456\u001b[0m, in \u001b[0;36mUOWTransaction.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rec \u001b[38;5;129;01min\u001b[39;00m topological\u001b[38;5;241m.\u001b[39msort(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdependencies, postsort_actions):\n\u001b[0;32m--> 456\u001b[0m         \u001b[43mrec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/papyrus/lib/python3.8/site-packages/sqlalchemy/orm/unitofwork.py:667\u001b[0m, in \u001b[0;36mDeleteAll.execute\u001b[0;34m(self, uow)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;129m@util\u001b[39m\u001b[38;5;241m.\u001b[39mpreload_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlalchemy.orm.persistence\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, uow):\n\u001b[0;32m--> 667\u001b[0m     \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreloaded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morm_persistence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete_obj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43muow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstates_for_mapper_hierarchy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43muow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/papyrus/lib/python3.8/site-packages/sqlalchemy/orm/persistence.py:343\u001b[0m, in \u001b[0;36mdelete_obj\u001b[0;34m(base_mapper, states, uowtransaction)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    339\u001b[0m     delete \u001b[38;5;241m=\u001b[39m _collect_delete_commands(\n\u001b[1;32m    340\u001b[0m         base_mapper, uowtransaction, table, states_to_delete\n\u001b[1;32m    341\u001b[0m     )\n\u001b[0;32m--> 343\u001b[0m     \u001b[43m_emit_delete_statements\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_mapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43muowtransaction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelete\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (\n\u001b[1;32m    352\u001b[0m     state,\n\u001b[1;32m    353\u001b[0m     state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    356\u001b[0m     update_version_id,\n\u001b[1;32m    357\u001b[0m ) \u001b[38;5;129;01min\u001b[39;00m states_to_delete:\n\u001b[1;32m    358\u001b[0m     mapper\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_delete(mapper, connection, state)\n",
      "File \u001b[0;32m/opt/conda/envs/papyrus/lib/python3.8/site-packages/sqlalchemy/orm/persistence.py:1468\u001b[0m, in \u001b[0;36m_emit_delete_statements\u001b[0;34m(base_mapper, uowtransaction, mapper, table, delete)\u001b[0m\n\u001b[1;32m   1464\u001b[0m         connection\u001b[38;5;241m.\u001b[39m_execute_20(\n\u001b[1;32m   1465\u001b[0m             statement, del_objects, execution_options\u001b[38;5;241m=\u001b[39mexecution_options\n\u001b[1;32m   1466\u001b[0m         )\n\u001b[1;32m   1467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1468\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_20\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdel_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_options\u001b[49m\n\u001b[1;32m   1470\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m need_version_id:\n\u001b[1;32m   1473\u001b[0m         only_warn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/papyrus/lib/python3.8/site-packages/sqlalchemy/engine/base.py:1631\u001b[0m, in \u001b[0;36mConnection._execute_20\u001b[0;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[1;32m   1627\u001b[0m     util\u001b[38;5;241m.\u001b[39mraise_(\n\u001b[1;32m   1628\u001b[0m         exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement), replace_context\u001b[38;5;241m=\u001b[39merr\n\u001b[1;32m   1629\u001b[0m     )\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_10style\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_10style\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/papyrus/lib/python3.8/site-packages/sqlalchemy/sql/elements.py:325\u001b[0m, in \u001b[0;36mClauseElement._execute_on_connection\u001b[0;34m(self, connection, multiparams, params, execution_options, _force)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_execute_on_connection\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m, connection, multiparams, params, execution_options, _force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    323\u001b[0m ):\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _force \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_execution:\n\u001b[0;32m--> 325\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_clauseelement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/papyrus/lib/python3.8/site-packages/sqlalchemy/engine/base.py:1498\u001b[0m, in \u001b[0;36mConnection._execute_clauseelement\u001b[0;34m(self, elem, multiparams, params, execution_options)\u001b[0m\n\u001b[1;32m   1486\u001b[0m compiled_cache \u001b[38;5;241m=\u001b[39m execution_options\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1487\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiled_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_compiled_cache\n\u001b[1;32m   1488\u001b[0m )\n\u001b[1;32m   1490\u001b[0m compiled_sql, extracted_params, cache_hit \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_compile_w_cache(\n\u001b[1;32m   1491\u001b[0m     dialect\u001b[38;5;241m=\u001b[39mdialect,\n\u001b[1;32m   1492\u001b[0m     compiled_cache\u001b[38;5;241m=\u001b[39mcompiled_cache,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     linting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mcompiler_linting \u001b[38;5;241m|\u001b[39m compiler\u001b[38;5;241m.\u001b[39mWARN_LINTING,\n\u001b[1;32m   1497\u001b[0m )\n\u001b[0;32m-> 1498\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_compiled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m    \u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextracted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_hit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_hit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_events:\n\u001b[1;32m   1511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_execute(\n\u001b[1;32m   1512\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1513\u001b[0m         elem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1517\u001b[0m         ret,\n\u001b[1;32m   1518\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/papyrus/lib/python3.8/site-packages/sqlalchemy/engine/base.py:1862\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1859\u001b[0m             branched\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m   1861\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1862\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/envs/papyrus/lib/python3.8/site-packages/sqlalchemy/engine/base.py:2047\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[1;32m   2043\u001b[0m         util\u001b[38;5;241m.\u001b[39mraise_(\n\u001b[1;32m   2044\u001b[0m             sqlalchemy_exception, with_traceback\u001b[38;5;241m=\u001b[39mexc_info[\u001b[38;5;241m2\u001b[39m], from_\u001b[38;5;241m=\u001b[39me\n\u001b[1;32m   2045\u001b[0m         )\n\u001b[1;32m   2046\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2047\u001b[0m         \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_traceback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2050\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reentrant_error\n",
      "File \u001b[0;32m/opt/conda/envs/papyrus/lib/python3.8/site-packages/sqlalchemy/util/compat.py:207\u001b[0m, in \u001b[0;36mraise_\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    204\u001b[0m     exception\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m replace_context\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# credit to\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# as the __traceback__ object creates a cycle\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m exception, replace_context, from_, with_traceback\n",
      "File \u001b[0;32m/opt/conda/envs/papyrus/lib/python3.8/site-packages/sqlalchemy/engine/base.py:1799\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1797\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1798\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1799\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_executemany\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1802\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parameters \u001b[38;5;129;01mand\u001b[39;00m context\u001b[38;5;241m.\u001b[39mno_parameters:\n\u001b[1;32m   1803\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39m_has_events:\n",
      "File \u001b[0;32m/opt/conda/envs/papyrus/lib/python3.8/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py:971\u001b[0m, in \u001b[0;36mPGDialect_psycopg2.do_executemany\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_psycopg2_extras()\u001b[38;5;241m.\u001b[39mexecute_batch(\n\u001b[1;32m    968\u001b[0m         cursor, statement, parameters, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    969\u001b[0m     )\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 971\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutemany\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/code')\n",
    "\n",
    "from database.models import (Protein, Organism, Classification, Molecule, Activity, ActivityType, Source, Quality, CID)\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import scoped_session\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import multiprocessing\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "def get_db_session():\n",
    "    engine = create_engine(\n",
    "        os.environ.get('SQLALCHEMY_URL'), convert_unicode=True,\n",
    "        pool_recycle=3600, pool_size=10)\n",
    "    db_session = scoped_session(sessionmaker(\n",
    "        autocommit=False, autoflush=False, bind=engine))\n",
    "    \n",
    "    return db_session\n",
    "\n",
    "    \n",
    "mols = []\n",
    "count = 0\n",
    "\n",
    "\n",
    "\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "    \n",
    "\n",
    "# get neccessary molecule data as list of tuples\n",
    "session = get_db_session()\n",
    "mol_data = session.query(Molecule.id, Molecule.smiles, Molecule.inchi_auxinfo).all()\n",
    "mol_ids = {}\n",
    "for t in tqdm(mol_data):\n",
    "    combo = f'{t[1]}_{t[2]}'\n",
    "    if not combo in mol_ids.keys():\n",
    "        mol_ids[combo] = [t[0]]\n",
    "    else:\n",
    "        mol_ids[combo].append(t[0])\n",
    "        \n",
    "count = 0\n",
    "dels = []\n",
    "for k in tqdm(mol_ids.keys()):\n",
    "    if len(mol_ids[k])>1:\n",
    "        to_del = mol_ids[k][1:]\n",
    "        dels.extend(to_del)\n",
    "        \n",
    "        \n",
    "for group in chunker(dels, 1000):\n",
    "    count +=1\n",
    "    print(f'{count*1000}/{len(dels)}')\n",
    "    mols.extend(session.query(Molecule).filter(Molecule.id.in_(group)).all())\n",
    "\n",
    "\n",
    "\n",
    "count = 0\n",
    "for group in chunker(mols, 100):\n",
    "    count +=1\n",
    "    print(f'{count*100}/{len(mols)}')\n",
    "    [session.delete(c) for c in group]\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1920a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8679fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/78698\n",
      "200/78698\n",
      "300/78698\n",
      "400/78698\n",
      "500/78698\n",
      "600/78698\n",
      "700/78698\n",
      "800/78698\n",
      "900/78698\n",
      "1000/78698\n",
      "1100/78698\n",
      "1200/78698\n",
      "1300/78698\n",
      "1400/78698\n",
      "1500/78698\n",
      "1600/78698\n",
      "1700/78698\n",
      "1800/78698\n",
      "1900/78698\n",
      "2000/78698\n",
      "2100/78698\n",
      "2200/78698\n",
      "2300/78698\n",
      "2400/78698\n",
      "2500/78698\n",
      "2600/78698\n",
      "2700/78698\n",
      "2800/78698\n",
      "2900/78698\n",
      "3000/78698\n",
      "3100/78698\n",
      "3200/78698\n",
      "3300/78698\n",
      "3400/78698\n",
      "3500/78698\n",
      "3600/78698\n",
      "3700/78698\n",
      "3800/78698\n",
      "3900/78698\n",
      "4000/78698\n",
      "4100/78698\n",
      "4200/78698\n",
      "4300/78698\n",
      "4400/78698\n",
      "4500/78698\n",
      "4600/78698\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for group in chunker(mols, 100):\n",
    "    count +=1\n",
    "    print(f'{count*100}/{len(mols)}')\n",
    "    [session.delete(c) for c in group]\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14095e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
